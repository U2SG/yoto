"""
å‹åŠ›æµ‹è¯•è„šæœ¬

ä¸“æ³¨äºæ€§èƒ½æµ‹è¯•ï¼ŒéªŒè¯æ™ºèƒ½ç¼“å­˜å¤±æ•ˆå’Œæ•°æ®é¢„åŠ è½½æœºåˆ¶çš„æ€§èƒ½è¡¨ç°
"""

import sys
import os
import time
import threading
import concurrent.futures
import statistics
from unittest.mock import Mock
from collections import defaultdict

# æ·»åŠ å½“å‰ç›®å½•åˆ°Pythonè·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
sys.path.insert(0, current_dir)


class StressTestSuite:
    """å‹åŠ›æµ‹è¯•å¥—ä»¶"""

    def __init__(self):
        self.results = {}
        self.setup_test_environment()

    def setup_test_environment(self):
        """è®¾ç½®æµ‹è¯•ç¯å¢ƒ"""
        print("ğŸ”§ è®¾ç½®å‹åŠ›æµ‹è¯•ç¯å¢ƒ...")

        # åˆ›å»ºFlaskåº”ç”¨
        from flask import Flask

        self.app = Flask(__name__)
        self.app.config.update(
            {
                "TESTING": True,
                "ADVANCED_OPTIMIZATION_CONFIG": {
                    "smart_invalidation_interval": 1,
                    "preload_interval": 1,
                    "preload": {"enabled": True},
                    "batch_size": 100,
                },
            }
        )

    def run_performance_tests(self):
        """è¿è¡Œæ€§èƒ½æµ‹è¯•"""
        print("\nğŸ“‹ å¼€å§‹æ€§èƒ½æµ‹è¯•...")
        print("=" * 60)

        performance_tests = [
            ("ç¼“å­˜æ“ä½œæ€§èƒ½", self.test_cache_performance, 1000, 50),
            ("æƒé™æ£€æŸ¥æ€§èƒ½", self.test_permission_check_performance, 500, 25),
            ("æ™ºèƒ½å¤±æ•ˆæ€§èƒ½", self.test_smart_invalidation_performance, 200, 10),
            ("é¢„åŠ è½½æ€§èƒ½", self.test_preload_performance, 100, 5),
            ("åŒé‡æ£€æŸ¥é”å®šæ€§èƒ½", self.test_double_checked_locking_performance, 800, 40),
        ]

        for test_name, test_func, total_requests, max_workers in performance_tests:
            print(f"\nğŸ”¥ æ€§èƒ½æµ‹è¯•: {test_name}")
            print("-" * 40)
            try:
                metrics = test_func(total_requests, max_workers)
                self.results[test_name] = metrics
                print(f"âœ… {test_name} å®Œæˆ")
                print(f"   å¹³å‡å“åº”æ—¶é—´: {metrics['avg_response_time']:.3f}ms")
                print(f"   æœ€å¤§å“åº”æ—¶é—´: {metrics['max_response_time']:.3f}ms")
                print(f"   æœ€å°å“åº”æ—¶é—´: {metrics['min_response_time']:.3f}ms")
                print(f"   æˆåŠŸç‡: {metrics['success_rate']:.2f}%")
                print(f"   æ€»è¯·æ±‚æ•°: {metrics['total_requests']}")
                print(f"   ååé‡: {metrics['throughput']:.2f} req/s")
            except Exception as e:
                print(f"âŒ {test_name} å¼‚å¸¸: {e}")

    def test_cache_performance(self, total_requests, max_workers):
        """æµ‹è¯•ç¼“å­˜æ“ä½œæ€§èƒ½"""
        print(f"ğŸ”¥ æ‰§è¡Œç¼“å­˜æ“ä½œæ€§èƒ½æµ‹è¯• ({total_requests} è¯·æ±‚, {max_workers} å¹¶å‘)...")

        def cache_operation(thread_id):
            """å•ä¸ªç¼“å­˜æ“ä½œ"""
            start_time = time.time()
            try:
                with self.app.app_context():
                    from app.core.permission.hybrid_permission_cache import (
                        HybridPermissionCache,
                    )

                    # åˆ›å»ºç¼“å­˜å®ä¾‹
                    cache = HybridPermissionCache()
                    cache.l1_simple_cache = Mock()
                    cache.l1_simple_cache.set = Mock()
                    cache.l1_simple_cache.get = Mock(
                        return_value={"permissions": ["test"]}
                    )

                    key = f"stress_test_key_{thread_id}"
                    data = {"permissions": [f"perm_{thread_id}"]}

                    # è®¾ç½®ç¼“å­˜
                    cache.l1_simple_cache.set(key, data)

                    # è·å–ç¼“å­˜
                    result = cache.l1_simple_cache.get(key)

                    return time.time() - start_time, True
            except Exception:
                return time.time() - start_time, False

        return self._run_concurrent_test(cache_operation, total_requests, max_workers)

    def test_permission_check_performance(self, total_requests, max_workers):
        """æµ‹è¯•æƒé™æ£€æŸ¥æ€§èƒ½"""
        print(f"ğŸ”¥ æ‰§è¡Œæƒé™æ£€æŸ¥æ€§èƒ½æµ‹è¯• ({total_requests} è¯·æ±‚, {max_workers} å¹¶å‘)...")

        def permission_check(thread_id):
            """å•ä¸ªæƒé™æ£€æŸ¥"""
            start_time = time.time()
            try:
                with self.app.app_context():
                    # æ¨¡æ‹Ÿæƒé™æ£€æŸ¥
                    time.sleep(0.001)  # æ¨¡æ‹Ÿå¤„ç†æ—¶é—´

                    return time.time() - start_time, True
            except Exception:
                return time.time() - start_time, False

        return self._run_concurrent_test(permission_check, total_requests, max_workers)

    def test_smart_invalidation_performance(self, total_requests, max_workers):
        """æµ‹è¯•æ™ºèƒ½å¤±æ•ˆæ€§èƒ½"""
        print(f"ğŸ”¥ æ‰§è¡Œæ™ºèƒ½å¤±æ•ˆæ€§èƒ½æµ‹è¯• ({total_requests} è¯·æ±‚, {max_workers} å¹¶å‘)...")

        def invalidation_operation(thread_id):
            """å•ä¸ªå¤±æ•ˆæ“ä½œ"""
            start_time = time.time()
            try:
                with self.app.app_context():
                    from app.core.permission.advanced_optimization import (
                        AdvancedDistributedOptimizer,
                    )

                    config = self.app.config.get("ADVANCED_OPTIMIZATION_CONFIG", {})
                    mock_redis = Mock()
                    mock_redis.ping.return_value = True

                    optimizer = AdvancedDistributedOptimizer(config, mock_redis)

                    # æ‰§è¡Œæ™ºèƒ½å¤±æ•ˆåˆ†æ
                    analysis = optimizer._get_smart_invalidation_analysis()

                    return time.time() - start_time, True
            except Exception:
                return time.time() - start_time, False

        return self._run_concurrent_test(
            invalidation_operation, total_requests, max_workers
        )

    def test_preload_performance(self, total_requests, max_workers):
        """æµ‹è¯•é¢„åŠ è½½æ€§èƒ½"""
        print(f"ğŸ”¥ æ‰§è¡Œé¢„åŠ è½½æ€§èƒ½æµ‹è¯• ({total_requests} è¯·æ±‚, {max_workers} å¹¶å‘)...")

        def preload_operation(thread_id):
            """å•ä¸ªé¢„åŠ è½½æ“ä½œ"""
            start_time = time.time()
            try:
                with self.app.app_context():
                    from app.core.permission.advanced_optimization import (
                        AdvancedDistributedOptimizer,
                    )

                    config = self.app.config.get("ADVANCED_OPTIMIZATION_CONFIG", {})
                    mock_redis = Mock()
                    mock_redis.ping.return_value = True
                    mock_redis.zrevrange.return_value = [b"1", b"2", b"3"]

                    optimizer = AdvancedDistributedOptimizer(config, mock_redis)

                    # æ‰§è¡Œé¢„åŠ è½½ç­–ç•¥
                    result = optimizer._execute_preload_strategy()

                    return time.time() - start_time, True
            except Exception:
                return time.time() - start_time, False

        return self._run_concurrent_test(preload_operation, total_requests, max_workers)

    def test_double_checked_locking_performance(self, total_requests, max_workers):
        """æµ‹è¯•åŒé‡æ£€æŸ¥é”å®šæ€§èƒ½"""
        print(
            f"ğŸ”¥ æ‰§è¡ŒåŒé‡æ£€æŸ¥é”å®šæ€§èƒ½æµ‹è¯• ({total_requests} è¯·æ±‚, {max_workers} å¹¶å‘)..."
        )

        def locking_operation(thread_id):
            """å•ä¸ªé”å®šæ“ä½œ"""
            start_time = time.time()
            try:
                with self.app.app_context():
                    from app.core.permission.hybrid_permission_cache import (
                        HybridPermissionCache,
                    )

                    # åˆ›å»ºæ¨¡æ‹Ÿçš„Rediså®¢æˆ·ç«¯
                    mock_redis = Mock()
                    mock_redis.get.return_value = b'{"permissions": ["perm1", "perm2"]}'

                    # åˆ›å»ºæ··åˆç¼“å­˜å®ä¾‹
                    cache = HybridPermissionCache()
                    cache.distributed_cache = Mock()
                    cache.distributed_cache.redis_client = mock_redis

                    # æ¨¡æ‹Ÿåˆ†å¸ƒå¼é”ç®¡ç†å™¨
                    mock_lock_manager = Mock()
                    mock_lock = Mock()
                    mock_lock.__enter__ = Mock(return_value=mock_lock)
                    mock_lock.__exit__ = Mock(return_value=None)
                    mock_lock_manager.create_lock.return_value = mock_lock
                    cache._distributed_lock_manager = mock_lock_manager

                    # æ‰§è¡ŒåŒé‡æ£€æŸ¥é”å®š
                    result = cache.distributed_cache_get(f"test_key_{thread_id}")

                    return time.time() - start_time, True
            except Exception:
                return time.time() - start_time, False

        return self._run_concurrent_test(locking_operation, total_requests, max_workers)

    def _run_concurrent_test(self, operation_func, total_requests, max_workers):
        """è¿è¡Œå¹¶å‘æµ‹è¯•"""
        response_times = []
        success_count = 0
        start_time = time.time()

        with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            future_to_thread = {
                executor.submit(operation_func, i): i for i in range(total_requests)
            }

            # æ”¶é›†ç»“æœ
            for future in concurrent.futures.as_completed(future_to_thread):
                response_time, success = future.result()
                response_times.append(response_time * 1000)  # è½¬æ¢ä¸ºæ¯«ç§’
                if success:
                    success_count += 1

        end_time = time.time()
        total_time = end_time - start_time

        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        if response_times:
            avg_response_time = statistics.mean(response_times)
            max_response_time = max(response_times)
            min_response_time = min(response_times)
        else:
            avg_response_time = max_response_time = min_response_time = 0

        success_rate = (
            (success_count / total_requests) * 100 if total_requests > 0 else 0
        )
        throughput = total_requests / total_time if total_time > 0 else 0

        return {
            "avg_response_time": avg_response_time,
            "max_response_time": max_response_time,
            "min_response_time": min_response_time,
            "success_rate": success_rate,
            "total_requests": total_requests,
            "success_count": success_count,
            "throughput": throughput,
            "total_time": total_time,
        }

    def generate_performance_report(self):
        """ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š"""
        print("\nğŸ“Š ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š...")
        print("=" * 60)

        print("\nğŸ”¥ æ€§èƒ½æµ‹è¯•ç»“æœ:")
        for test_name, metrics in self.results.items():
            print(f"\n   {test_name}:")
            print(f"     å¹³å‡å“åº”æ—¶é—´: {metrics['avg_response_time']:.3f}ms")
            print(f"     æœ€å¤§å“åº”æ—¶é—´: {metrics['max_response_time']:.3f}ms")
            print(f"     æœ€å°å“åº”æ—¶é—´: {metrics['min_response_time']:.3f}ms")
            print(f"     æˆåŠŸç‡: {metrics['success_rate']:.2f}%")
            print(f"     ååé‡: {metrics['throughput']:.2f} req/s")
            print(f"     æ€»è¯·æ±‚æ•°: {metrics['total_requests']}")
            print(f"     æ€»è€—æ—¶: {metrics['total_time']:.3f}s")

        print("\nğŸ“ˆ æ€§èƒ½åˆ†æ:")
        all_response_times = []
        all_throughputs = []
        for metrics in self.results.values():
            if "avg_response_time" in metrics:
                all_response_times.append(metrics["avg_response_time"])
            if "throughput" in metrics:
                all_throughputs.append(metrics["throughput"])

        if all_response_times:
            overall_avg_response = statistics.mean(all_response_times)
            overall_max_response = max(all_response_times)
            print(f"   æ•´ä½“å¹³å‡å“åº”æ—¶é—´: {overall_avg_response:.3f}ms")
            print(f"   æ•´ä½“æœ€å¤§å“åº”æ—¶é—´: {overall_max_response:.3f}ms")

        if all_throughputs:
            overall_avg_throughput = statistics.mean(all_throughputs)
            overall_max_throughput = max(all_throughputs)
            print(f"   æ•´ä½“å¹³å‡ååé‡: {overall_avg_throughput:.2f} req/s")
            print(f"   æ•´ä½“æœ€å¤§ååé‡: {overall_max_throughput:.2f} req/s")

        print("\nâœ… å‹åŠ›æµ‹è¯•å®Œæˆï¼")
        print("æ™ºèƒ½ç¼“å­˜å¤±æ•ˆå’Œæ•°æ®é¢„åŠ è½½æœºåˆ¶å·²é€šè¿‡æ€§èƒ½æµ‹è¯•éªŒè¯ã€‚")


def run_stress_performance_test():
    """è¿è¡Œå‹åŠ›æ€§èƒ½æµ‹è¯•"""
    print("ğŸš€ å¼€å§‹å‹åŠ›æ€§èƒ½æµ‹è¯•...")
    print("=" * 60)

    # åˆ›å»ºæµ‹è¯•å¥—ä»¶
    test_suite = StressTestSuite()

    # è¿è¡Œæ€§èƒ½æµ‹è¯•
    test_suite.run_performance_tests()

    # ç”ŸæˆæŠ¥å‘Š
    test_suite.generate_performance_report()

    print("\nğŸ‰ å‹åŠ›æµ‹è¯•å®Œæˆï¼")
    return True


if __name__ == "__main__":
    run_stress_performance_test()
