#!/usr/bin/env python3
"""
å•ç”¨æˆ·æŸ¥è¯¢ç“¶é¢ˆåˆ†æå·¥å…·
åˆ†æç¼“å­˜è®¿é—®è·¯å¾„ã€æƒé™èšåˆç®—æ³•ã€æ•°æ®åº“æŸ¥è¯¢ä¼˜åŒ–
"""
import sys
import os
import time
import statistics
import cProfile
import pstats
import io
from typing import Dict, List, Set, Tuple

# æ·»åŠ çˆ¶ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), ".."))

from app import create_app
from app.core.extensions import db
from app.blueprints.auth.models import User
from app.blueprints.roles.models import Role, UserRole, RolePermission, Permission
from app.core.permissions import (
    _optimized_single_user_query_v2,
    _precompute_user_permissions,
    _get_permissions_from_cache,
    _set_permissions_to_cache,
    _make_perm_cache_key,
    _permission_cache,
    _precomputed_permissions,
)


class BottleneckAnalyzer:
    """ç“¶é¢ˆåˆ†æå™¨"""

    def __init__(self):
        self.app = create_app("mysql_testing")
        self.setup_database()

        # åˆ†æç»“æœ
        self.analysis_results = {
            "cache_access": {},
            "permission_aggregation": {},
            "database_query": {},
            "overall_performance": {},
        }

    def setup_database(self):
        """è®¾ç½®æ•°æ®åº“"""
        with self.app.app_context():
            try:
                db.create_all()
                self.create_test_data()
            except Exception as e:
                print(f"æ•°æ®åº“è®¾ç½®å¤±è´¥: {e}")

    def create_test_data(self):
        """åˆ›å»ºæµ‹è¯•æ•°æ®"""
        print("åˆ›å»ºç“¶é¢ˆåˆ†ææµ‹è¯•æ•°æ®...")

        import time

        unique_id = int(time.time() * 1000) % 100000

        # åˆ›å»ºç”¨æˆ·
        user = User(username=f"bottleneck_user_{unique_id}", password_hash="test_hash")
        db.session.add(user)
        db.session.commit()
        self.test_user_id = user.id

        # åˆ›å»ºè§’è‰²
        roles = []
        for i in range(10):  # 10ä¸ªè§’è‰²
            role = Role(name=f"bottleneck_role_{unique_id}_{i}", server_id=1)
            roles.append(role)
        db.session.add_all(roles)
        db.session.commit()

        # åˆ›å»ºæƒé™
        permissions = []
        for i in range(100):  # 100ä¸ªæƒé™
            perm = Permission(
                name=f"bottleneck_perm_{unique_id}_{i}",
                group="bottleneck",
                description=f"Bottleneck permission {i}",
            )
            permissions.append(perm)
        db.session.add_all(permissions)
        db.session.commit()

        # åˆ†é…ç”¨æˆ·è§’è‰²
        user_roles = []
        for role in roles:
            user_role = UserRole(user_id=user.id, role_id=role.id)
            user_roles.append(user_role)
        db.session.add_all(user_roles)
        db.session.commit()

        # åˆ†é…è§’è‰²æƒé™
        role_permissions = []
        for i, role in enumerate(roles):
            for j in range(10):  # æ¯ä¸ªè§’è‰²10ä¸ªæƒé™
                perm = permissions[i * 10 + j]
                rp = RolePermission(role_id=role.id, permission_id=perm.id)
                role_permissions.append(rp)
        db.session.add_all(role_permissions)
        db.session.commit()

        print(
            f"åˆ›å»ºäº†æµ‹è¯•ç”¨æˆ· {user.id}, {len(roles)} ä¸ªè§’è‰², {len(permissions)} ä¸ªæƒé™"
        )

    def analyze_cache_access_path(self):
        """åˆ†æç¼“å­˜è®¿é—®è·¯å¾„ç“¶é¢ˆ"""
        print("\n=== ç¼“å­˜è®¿é—®è·¯å¾„åˆ†æ ===")

        cache_metrics = {
            "l1_cache_hit_time": [],
            "l1_cache_miss_time": [],
            "l2_cache_hit_time": [],
            "l2_cache_miss_time": [],
            "cache_key_generation_time": [],
            "serialization_time": [],
            "deserialization_time": [],
        }

        with self.app.app_context():
            # 1. åˆ†æç¼“å­˜é”®ç”Ÿæˆæ€§èƒ½
            print("1. åˆ†æç¼“å­˜é”®ç”Ÿæˆæ€§èƒ½...")
            for i in range(1000):
                start_time = time.time()
                cache_key = _make_perm_cache_key(self.test_user_id, "server", 1)
                key_gen_time = time.time() - start_time
                cache_metrics["cache_key_generation_time"].append(key_gen_time)

            # 2. åˆ†æL1ç¼“å­˜è®¿é—®æ€§èƒ½
            print("2. åˆ†æL1ç¼“å­˜è®¿é—®æ€§èƒ½...")
            # å…ˆå¡«å……ç¼“å­˜
            permissions = {"test_perm_1", "test_perm_2", "test_perm_3"}
            cache_key = _make_perm_cache_key(self.test_user_id, "server", 1)
            _permission_cache.set(cache_key, permissions)

            # æµ‹è¯•L1ç¼“å­˜å‘½ä¸­
            for i in range(1000):
                start_time = time.time()
                result = _permission_cache.get(cache_key)
                hit_time = time.time() - start_time
                cache_metrics["l1_cache_hit_time"].append(hit_time)

            # æµ‹è¯•L1ç¼“å­˜æœªå‘½ä¸­
            for i in range(1000):
                start_time = time.time()
                result = _permission_cache.get(f"miss_key_{i}")
                miss_time = time.time() - start_time
                cache_metrics["l1_cache_miss_time"].append(miss_time)

            # 3. åˆ†æåºåˆ—åŒ–/ååºåˆ—åŒ–æ€§èƒ½
            print("3. åˆ†æåºåˆ—åŒ–/ååºåˆ—åŒ–æ€§èƒ½...")
            from app.core.permissions import (
                _serialize_permissions,
                _deserialize_permissions,
            )

            for i in range(1000):
                test_permissions = {f"perm_{j}" for j in range(50)}

                # åºåˆ—åŒ–æµ‹è¯•
                start_time = time.time()
                serialized = _serialize_permissions(test_permissions)
                serialize_time = time.time() - start_time
                cache_metrics["serialization_time"].append(serialize_time)

                # ååºåˆ—åŒ–æµ‹è¯•
                start_time = time.time()
                deserialized = _deserialize_permissions(serialized)
                deserialize_time = time.time() - start_time
                cache_metrics["deserialization_time"].append(deserialize_time)

        # è®¡ç®—ç»Ÿè®¡ç»“æœ
        cache_results = {}
        for metric, times in cache_metrics.items():
            if times:
                cache_results[metric] = {
                    "avg_time": statistics.mean(times),
                    "min_time": min(times),
                    "max_time": max(times),
                    "std_dev": statistics.stdev(times) if len(times) > 1 else 0,
                }

        self.analysis_results["cache_access"] = cache_results
        return cache_results

    def analyze_permission_aggregation(self):
        """åˆ†ææƒé™èšåˆç®—æ³•ç“¶é¢ˆ"""
        print("\n=== æƒé™èšåˆç®—æ³•åˆ†æ ===")

        aggregation_metrics = {
            "role_collection_time": [],
            "permission_collection_time": [],
            "inheritance_resolution_time": [],
            "scope_filtering_time": [],
            "set_operations_time": [],
        }

        with self.app.app_context():
            # 1. åˆ†æè§’è‰²æ”¶é›†æ€§èƒ½
            print("1. åˆ†æè§’è‰²æ”¶é›†æ€§èƒ½...")
            for i in range(100):
                start_time = time.time()
                roles = (
                    db.session.query(UserRole.role_id)
                    .join(Role, UserRole.role_id == Role.id)
                    .filter(
                        UserRole.user_id == self.test_user_id,
                        Role.is_active == True,
                        Role.deleted_at.is_(None),
                    )
                    .all()
                )
                role_time = time.time() - start_time
                aggregation_metrics["role_collection_time"].append(role_time)

            # 2. åˆ†ææƒé™æ”¶é›†æ€§èƒ½
            print("2. åˆ†ææƒé™æ”¶é›†æ€§èƒ½...")
            role_ids = [
                r[0]
                for r in db.session.query(UserRole.role_id)
                .filter(UserRole.user_id == self.test_user_id)
                .all()
            ]

            for i in range(100):
                start_time = time.time()
                permissions = (
                    db.session.query(Permission.name)
                    .join(RolePermission, Permission.id == RolePermission.permission_id)
                    .filter(
                        RolePermission.role_id.in_(role_ids),
                        Permission.is_deprecated == False,
                    )
                    .distinct()
                    .all()
                )
                perm_time = time.time() - start_time
                aggregation_metrics["permission_collection_time"].append(perm_time)

            # 3. åˆ†æä½œç”¨åŸŸè¿‡æ»¤æ€§èƒ½
            print("3. åˆ†æä½œç”¨åŸŸè¿‡æ»¤æ€§èƒ½...")
            for i in range(100):
                start_time = time.time()
                scoped_permissions = (
                    db.session.query(Permission.name)
                    .join(RolePermission, Permission.id == RolePermission.permission_id)
                    .filter(
                        RolePermission.role_id.in_(role_ids),
                        RolePermission.scope_type == "server",
                        RolePermission.scope_id == 1,
                        Permission.is_deprecated == False,
                    )
                    .distinct()
                    .all()
                )
                scope_time = time.time() - start_time
                aggregation_metrics["scope_filtering_time"].append(scope_time)

            # 4. åˆ†æé›†åˆæ“ä½œæ€§èƒ½
            print("4. åˆ†æé›†åˆæ“ä½œæ€§èƒ½...")
            for i in range(1000):
                set1 = {f"perm_{j}" for j in range(50)}
                set2 = {f"perm_{j+25}" for j in range(50)}

                start_time = time.time()
                union_result = set1.union(set2)
                intersection_result = set1.intersection(set2)
                difference_result = set1.difference(set2)
                set_time = time.time() - start_time
                aggregation_metrics["set_operations_time"].append(set_time)

        # è®¡ç®—ç»Ÿè®¡ç»“æœ
        aggregation_results = {}
        for metric, times in aggregation_metrics.items():
            if times:
                aggregation_results[metric] = {
                    "avg_time": statistics.mean(times),
                    "min_time": min(times),
                    "max_time": max(times),
                    "std_dev": statistics.stdev(times) if len(times) > 1 else 0,
                }

        self.analysis_results["permission_aggregation"] = aggregation_results
        return aggregation_results

    def analyze_database_query(self):
        """åˆ†ææ•°æ®åº“æŸ¥è¯¢ç“¶é¢ˆ"""
        print("\n=== æ•°æ®åº“æŸ¥è¯¢åˆ†æ ===")

        query_metrics = {
            "join_query_time": [],
            "subquery_time": [],
            "index_usage_time": [],
            "distinct_operation_time": [],
            "filter_operation_time": [],
        }

        with self.app.app_context():
            # 1. åˆ†æJOINæŸ¥è¯¢æ€§èƒ½
            print("1. åˆ†æJOINæŸ¥è¯¢æ€§èƒ½...")
            for i in range(50):
                start_time = time.time()
                query = (
                    db.session.query(Permission.name)
                    .join(RolePermission, Permission.id == RolePermission.permission_id)
                    .join(UserRole, RolePermission.role_id == UserRole.role_id)
                    .join(Role, UserRole.role_id == Role.id)
                    .filter(
                        UserRole.user_id == self.test_user_id,
                        Role.is_active == True,
                        Role.deleted_at.is_(None),
                        Permission.is_deprecated == False,
                    )
                    .distinct()
                )
                result = query.all()
                join_time = time.time() - start_time
                query_metrics["join_query_time"].append(join_time)

            # 2. åˆ†æå­æŸ¥è¯¢æ€§èƒ½
            print("2. åˆ†æå­æŸ¥è¯¢æ€§èƒ½...")
            for i in range(50):
                start_time = time.time()
                valid_roles_subquery = (
                    db.session.query(UserRole.role_id)
                    .join(Role, UserRole.role_id == Role.id)
                    .filter(
                        UserRole.user_id == self.test_user_id,
                        Role.is_active == True,
                        Role.deleted_at.is_(None),
                    )
                    .subquery()
                )

                query = (
                    db.session.query(Permission.name)
                    .join(RolePermission, Permission.id == RolePermission.permission_id)
                    .filter(
                        RolePermission.role_id.in_(valid_roles_subquery),
                        Permission.is_deprecated == False,
                    )
                    .distinct()
                )
                result = query.all()
                subquery_time = time.time() - start_time
                query_metrics["subquery_time"].append(subquery_time)

            # 3. åˆ†æDISTINCTæ“ä½œæ€§èƒ½
            print("3. åˆ†æDISTINCTæ“ä½œæ€§èƒ½...")
            for i in range(100):
                start_time = time.time()
                query = db.session.query(Permission.name).distinct()
                result = query.all()
                distinct_time = time.time() - start_time
                query_metrics["distinct_operation_time"].append(distinct_time)

            # 4. åˆ†æè¿‡æ»¤æ“ä½œæ€§èƒ½
            print("4. åˆ†æè¿‡æ»¤æ“ä½œæ€§èƒ½...")
            for i in range(100):
                start_time = time.time()
                query = db.session.query(Permission.name).filter(
                    Permission.is_deprecated == False
                )
                result = query.all()
                filter_time = time.time() - start_time
                query_metrics["filter_operation_time"].append(filter_time)

        # è®¡ç®—ç»Ÿè®¡ç»“æœ
        query_results = {}
        for metric, times in query_metrics.items():
            if times:
                query_results[metric] = {
                    "avg_time": statistics.mean(times),
                    "min_time": min(times),
                    "max_time": max(times),
                    "std_dev": statistics.stdev(times) if len(times) > 1 else 0,
                }

        self.analysis_results["database_query"] = query_results
        return query_results

    def profile_optimized_query(self):
        """ä½¿ç”¨cProfileåˆ†æä¼˜åŒ–æŸ¥è¯¢çš„æ€§èƒ½"""
        print("\n=== æ€§èƒ½åˆ†æå™¨åˆ†æ ===")

        with self.app.app_context():
            # åˆ›å»ºæ€§èƒ½åˆ†æå™¨
            profiler = cProfile.Profile()
            profiler.enable()

            # æ‰§è¡Œä¼˜åŒ–æŸ¥è¯¢
            for i in range(100):
                permissions = _optimized_single_user_query_v2(
                    self.test_user_id, "server", 1
                )

            profiler.disable()

            # è·å–åˆ†æç»“æœ
            s = io.StringIO()
            ps = pstats.Stats(profiler, stream=s).sort_stats("cumulative")
            ps.print_stats(20)  # æ˜¾ç¤ºå‰20ä¸ªæœ€è€—æ—¶çš„å‡½æ•°

            profile_output = s.getvalue()
            print("æ€§èƒ½åˆ†æç»“æœ:")
            print(profile_output)

            return profile_output

    def identify_bottlenecks(self):
        """è¯†åˆ«ä¸»è¦ç“¶é¢ˆ"""
        print("\n=== ç“¶é¢ˆè¯†åˆ« ===")

        bottlenecks = []

        # åˆ†æç¼“å­˜ç“¶é¢ˆ
        cache_results = self.analysis_results["cache_access"]
        if cache_results:
            # æ£€æŸ¥ç¼“å­˜é”®ç”Ÿæˆæ˜¯å¦è¿‡æ…¢
            if "cache_key_generation_time" in cache_results:
                avg_key_time = cache_results["cache_key_generation_time"]["avg_time"]
                if avg_key_time > 0.001:  # è¶…è¿‡1ms
                    bottlenecks.append(f"ç¼“å­˜é”®ç”Ÿæˆè¿‡æ…¢: {avg_key_time:.6f}s")

            # æ£€æŸ¥åºåˆ—åŒ–æ˜¯å¦è¿‡æ…¢
            if "serialization_time" in cache_results:
                avg_serialize_time = cache_results["serialization_time"]["avg_time"]
                if avg_serialize_time > 0.001:  # è¶…è¿‡1ms
                    bottlenecks.append(f"åºåˆ—åŒ–è¿‡æ…¢: {avg_serialize_time:.6f}s")

        # åˆ†ææƒé™èšåˆç“¶é¢ˆ
        aggregation_results = self.analysis_results["permission_aggregation"]
        if aggregation_results:
            # æ£€æŸ¥è§’è‰²æ”¶é›†æ˜¯å¦è¿‡æ…¢
            if "role_collection_time" in aggregation_results:
                avg_role_time = aggregation_results["role_collection_time"]["avg_time"]
                if avg_role_time > 0.01:  # è¶…è¿‡10ms
                    bottlenecks.append(f"è§’è‰²æ”¶é›†è¿‡æ…¢: {avg_role_time:.6f}s")

            # æ£€æŸ¥æƒé™æ”¶é›†æ˜¯å¦è¿‡æ…¢
            if "permission_collection_time" in aggregation_results:
                avg_perm_time = aggregation_results["permission_collection_time"][
                    "avg_time"
                ]
                if avg_perm_time > 0.01:  # è¶…è¿‡10ms
                    bottlenecks.append(f"æƒé™æ”¶é›†è¿‡æ…¢: {avg_perm_time:.6f}s")

        # åˆ†ææ•°æ®åº“æŸ¥è¯¢ç“¶é¢ˆ
        query_results = self.analysis_results["database_query"]
        if query_results:
            # æ£€æŸ¥JOINæŸ¥è¯¢æ˜¯å¦è¿‡æ…¢
            if "join_query_time" in query_results:
                avg_join_time = query_results["join_query_time"]["avg_time"]
                if avg_join_time > 0.05:  # è¶…è¿‡50ms
                    bottlenecks.append(f"JOINæŸ¥è¯¢è¿‡æ…¢: {avg_join_time:.6f}s")

            # æ£€æŸ¥å­æŸ¥è¯¢æ˜¯å¦è¿‡æ…¢
            if "subquery_time" in query_results:
                avg_subquery_time = query_results["subquery_time"]["avg_time"]
                if avg_subquery_time > 0.05:  # è¶…è¿‡50ms
                    bottlenecks.append(f"å­æŸ¥è¯¢è¿‡æ…¢: {avg_subquery_time:.6f}s")

        return bottlenecks

    def generate_optimization_recommendations(self):
        """ç”Ÿæˆä¼˜åŒ–å»ºè®®"""
        print("\n=== ä¼˜åŒ–å»ºè®® ===")

        recommendations = []
        bottlenecks = self.identify_bottlenecks()

        for bottleneck in bottlenecks:
            if "ç¼“å­˜é”®ç”Ÿæˆè¿‡æ…¢" in bottleneck:
                recommendations.append(
                    {
                        "area": "cache_access",
                        "issue": "ç¼“å­˜é”®ç”Ÿæˆè¿‡æ…¢",
                        "solution": "ä½¿ç”¨æ›´ç®€å•çš„é”®ç”Ÿæˆç®—æ³•ï¼Œé¿å…MD5å“ˆå¸Œ",
                        "priority": "high",
                    }
                )

            elif "åºåˆ—åŒ–è¿‡æ…¢" in bottleneck:
                recommendations.append(
                    {
                        "area": "cache_access",
                        "issue": "åºåˆ—åŒ–è¿‡æ…¢",
                        "solution": "ä½¿ç”¨æ›´å¿«çš„åºåˆ—åŒ–æ ¼å¼ï¼Œå¦‚pickleæˆ–msgpack",
                        "priority": "medium",
                    }
                )

            elif "è§’è‰²æ”¶é›†è¿‡æ…¢" in bottleneck:
                recommendations.append(
                    {
                        "area": "permission_aggregation",
                        "issue": "è§’è‰²æ”¶é›†è¿‡æ…¢",
                        "solution": "æ·»åŠ è§’è‰²ç¼“å­˜ï¼Œå‡å°‘æ•°æ®åº“æŸ¥è¯¢",
                        "priority": "high",
                    }
                )

            elif "æƒé™æ”¶é›†è¿‡æ…¢" in bottleneck:
                recommendations.append(
                    {
                        "area": "permission_aggregation",
                        "issue": "æƒé™æ”¶é›†è¿‡æ…¢",
                        "solution": "ä½¿ç”¨æ‰¹é‡æŸ¥è¯¢ï¼Œå‡å°‘JOINæ“ä½œ",
                        "priority": "high",
                    }
                )

            elif "JOINæŸ¥è¯¢è¿‡æ…¢" in bottleneck:
                recommendations.append(
                    {
                        "area": "database_query",
                        "issue": "JOINæŸ¥è¯¢è¿‡æ…¢",
                        "solution": "ä¼˜åŒ–ç´¢å¼•ï¼Œä½¿ç”¨è¦†ç›–ç´¢å¼•ï¼Œå‡å°‘JOINè¡¨æ•°é‡",
                        "priority": "high",
                    }
                )

            elif "å­æŸ¥è¯¢è¿‡æ…¢" in bottleneck:
                recommendations.append(
                    {
                        "area": "database_query",
                        "issue": "å­æŸ¥è¯¢è¿‡æ…¢",
                        "solution": "ä½¿ç”¨EXISTSæ›¿ä»£INå­æŸ¥è¯¢ï¼Œä¼˜åŒ–ç´¢å¼•",
                        "priority": "medium",
                    }
                )

        return recommendations

    def run_full_analysis(self):
        """è¿è¡Œå®Œæ•´åˆ†æ"""
        print("å¼€å§‹å•ç”¨æˆ·æŸ¥è¯¢ç“¶é¢ˆåˆ†æ...")
        print("=" * 60)

        # 1. ç¼“å­˜è®¿é—®è·¯å¾„åˆ†æ
        self.analyze_cache_access_path()

        # 2. æƒé™èšåˆç®—æ³•åˆ†æ
        self.analyze_permission_aggregation()

        # 3. æ•°æ®åº“æŸ¥è¯¢åˆ†æ
        self.analyze_database_query()

        # 4. æ€§èƒ½åˆ†æå™¨åˆ†æ
        profile_output = self.profile_optimized_query()

        # 5. è¯†åˆ«ç“¶é¢ˆ
        bottlenecks = self.identify_bottlenecks()

        # 6. ç”Ÿæˆä¼˜åŒ–å»ºè®®
        recommendations = self.generate_optimization_recommendations()

        # 7. ç”Ÿæˆåˆ†ææŠ¥å‘Š
        self.generate_analysis_report(bottlenecks, recommendations, profile_output)

    def generate_analysis_report(self, bottlenecks, recommendations, profile_output):
        """ç”Ÿæˆåˆ†ææŠ¥å‘Š"""
        print("\n" + "=" * 60)
        print("å•ç”¨æˆ·æŸ¥è¯¢ç“¶é¢ˆåˆ†ææŠ¥å‘Š")
        print("=" * 60)

        print(f"\n1. å‘ç°çš„ç“¶é¢ˆ ({len(bottlenecks)} ä¸ª):")
        for i, bottleneck in enumerate(bottlenecks, 1):
            print(f"   {i}. {bottleneck}")

        print(f"\n2. ä¼˜åŒ–å»ºè®® ({len(recommendations)} ä¸ª):")
        for i, rec in enumerate(recommendations, 1):
            priority_icon = (
                "ğŸ”´"
                if rec["priority"] == "high"
                else "ğŸŸ¡" if rec["priority"] == "medium" else "ğŸŸ¢"
            )
            print(f"   {i}. {priority_icon} {rec['area']}: {rec['issue']}")
            print(f"      è§£å†³æ–¹æ¡ˆ: {rec['solution']}")

        print(f"\n3. æ€§èƒ½æŒ‡æ ‡:")
        for area, results in self.analysis_results.items():
            if results:
                print(f"   {area}:")
                for metric, stats in results.items():
                    if isinstance(stats, dict) and "avg_time" in stats:
                        print(f"     - {metric}: {stats['avg_time']:.6f}s (avg)")

        print(f"\n4. å…³é”®å‘ç°:")
        if bottlenecks:
            print("   âš ï¸  å‘ç°æ€§èƒ½ç“¶é¢ˆï¼Œéœ€è¦ä¼˜åŒ–")
        else:
            print("   âœ… æœªå‘ç°æ˜æ˜¾ç“¶é¢ˆ")

        # ä¿å­˜è¯¦ç»†æŠ¥å‘Š
        self.save_detailed_report(bottlenecks, recommendations, profile_output)

    def save_detailed_report(self, bottlenecks, recommendations, profile_output):
        """ä¿å­˜è¯¦ç»†æŠ¥å‘Š"""
        report_content = f"""
# å•ç”¨æˆ·æŸ¥è¯¢ç“¶é¢ˆåˆ†æè¯¦ç»†æŠ¥å‘Š

## å‘ç°çš„ç“¶é¢ˆ
{chr(10).join(f"- {bottleneck}" for bottleneck in bottlenecks)}

## ä¼˜åŒ–å»ºè®®
{chr(10).join(f"- **{rec['area']}**: {rec['issue']} -> {rec['solution']}" for rec in recommendations)}

## æ€§èƒ½åˆ†æå™¨è¾“å‡º
```
{profile_output}
```

## è¯¦ç»†æ€§èƒ½æŒ‡æ ‡
"""

        for area, results in self.analysis_results.items():
            if results:
                report_content += f"\n### {area}\n"
                for metric, stats in results.items():
                    if isinstance(stats, dict) and "avg_time" in stats:
                        report_content += f"- {metric}: {stats['avg_time']:.6f}s (avg), {stats['min_time']:.6f}s (min), {stats['max_time']:.6f}s (max)\n"

        with open("bottleneck_analysis_report.md", "w", encoding="utf-8") as f:
            f.write(report_content)

        print("è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ°: bottleneck_analysis_report.md")

    def cleanup(self):
        """æ¸…ç†æµ‹è¯•æ•°æ®"""
        with self.app.app_context():
            try:
                from sqlalchemy import text

                db.session.execute(text("SET FOREIGN_KEY_CHECKS = 0"))
                db.session.execute(text("DROP TABLE IF EXISTS role_permissions"))
                db.session.execute(text("DROP TABLE IF EXISTS user_roles"))
                db.session.execute(text("DROP TABLE IF EXISTS permissions"))
                db.session.execute(text("DROP TABLE IF EXISTS roles"))
                db.session.execute(text("DROP TABLE IF EXISTS users"))
                db.session.execute(text("SET FOREIGN_KEY_CHECKS = 1"))
                db.session.commit()
                print("æµ‹è¯•æ•°æ®æ¸…ç†å®Œæˆ")
            except Exception as e:
                print(f"æ¸…ç†å¤±è´¥: {e}")


def main():
    """ä¸»å‡½æ•°"""
    analyzer = BottleneckAnalyzer()

    try:
        analyzer.run_full_analysis()
    finally:
        analyzer.cleanup()


if __name__ == "__main__":
    main()
